INSERT INTO `quality` (`name`, `type`, `formula`, `description`, `min`, `max`, `unit`) VALUES
('BiasVarianceProfile', 'AlgorithmQuality', 'Calculated using 10 iterations of Weka''s \r\nweka.BVDecomposeSegCVSub(1.7) bias-variance decomposition procedure', 'The weight of the bias component in the learning algorithm''s error. I.e., the percentage of errors that can be attributed to bias error (underfitting) as opposed to variance error (overfitting).', 0, 1, NULL),
('BiasWeightKohaviWolpert', 'AlgorithmQuality', NULL, 'empirically calculated average ratio of bias error in the total error, using Kohavi-Wolpert''s definition of bias and variance', NULL, NULL, NULL),
('BiasWeightWebb', 'AlgorithmQuality', NULL, 'empirically determined average ratio of bias error in the total error, using Webb''s definition of bias and variance', NULL, NULL, NULL),
('DefaultAccuracy', 'DataQuality', 'Predictive performance of the weka.ZeroR algorithm', 'The predictive accuracy obtained by simply predicting the majority class.', NULL, NULL, NULL),
('EntropyClass', 'DataQuality', 'See Entropy.', 'Entropy of the class attribute. It determines the amount of information needed to specify the class of an instance, or how `informative'' the attributes need to be. A low class entropy means that the distribution of examples among classes is very skewed (containing some very infrequent classes) which some algorithms cannot handle well. \r\n', NULL, NULL, NULL),
('EquivalentNumberOfFeatures', 'DataQuality', '\\frac{H(C)}{\\overline{MI(C,X)}}\r\n\r\nH(C) is the entropy of the class attribute\r\nMI(C,X) is the mutual information between attribute X and the class attribute C', 'The equivalent number of features is a quick estimate of the number of attributes required, on average, to describe the class (assuming independence).', NULL, NULL, NULL),
('FeatureAbsoluteSkewness', 'DataQuality', 'abs(skewness attribute i) over all attributes. Usually, the min,max and mean are calculated.', 'Absolute skewness values over all features. Usually, the min,max and mean are calculated. Skewness is a measure of how non-normal a feature''s value distribution is. Many learning algorithms assume normality.', NULL, NULL, NULL),
('FeatureEntropy', 'DataQuality', 'Entropies of all features, see EntropyOfClassAttribute. Usually, the min,max and mean are calculated.', 'Mean of entropies per attribute. See EntropyOfClassAttribute. Usually, the min,max and mean are calculated.', NULL, NULL, NULL),
('FeatureKurtosis', 'DataQuality', 'Kurtosis:\r\n***Latex***\r\n\\beta=\\frac{E(X-\\mu_{X})^{4}}{\\sigma_{X}^{4}}\r\n\r\n***Java***\r\ndouble[] mom = new double[$nrmoments];\r\nfor (int m = 0; m < $nrmoments; m++) {\r\n if ($count[i] > 0) \r\n  $moments[i][m] = $moments[i][m] / $count[i];\r\n if (m==0) mom[0] = $moments[i][0]; \r\n else mom[m] = power($moments[i], -mom[0],m+1);\r\ndouble kurt = mom[3] / Math.pow(mom[1],2) - 3;\r\n', 'Kurtosis over all features. Usually, the min,max and mean are calculated. Kurtosis measures the ''fatness'' of the value distribution''s tail. ', NULL, NULL, NULL),
('FeatureSkewness', 'DataQuality', 'Skewness:\r\n***Latex***\r\n\\gamma=\\frac{E(X-\\mu_{X})^{3}}{\\sigma_{X}^{3}\r\n\r\n***Java***\r\ndouble[] mom = new double[$nrmoments];\r\nfor (int m = 0; m < $nrmoments; m++) {\r\n if ($count[i] > 0) \r\n  $moments[i][m] = $moments[i][m] / $count[i];\r\n if (m==0) mom[0] = $moments[i][0]; \r\n else mom[m] = power($moments[i], -mom[0],m+1);\r\ndouble skew = mom[2] / Math.pow(mom[1],1.5);', 'Skewness over all features. Usually, the min,max and mean are calculated. Skewness is a measure of how non-normal a feature''s value distribution is. Many learning algorithms assume normality. Negative skew: longer left tail, distribution is left-skewed.\r\nPositive skew: longer right tail, distribution is right-skewed. ', NULL, NULL, NULL),
('HandlesMissingValues', 'AlgorithmQuality', NULL, NULL, NULL, NULL, NULL),
('HandlesNominalFeatures', 'AlgorithmQuality', NULL, NULL, NULL, NULL, NULL),
('HandlesNominalTarget', 'AlgorithmQuality', NULL, NULL, NULL, NULL, NULL),
('HandlesNonBinaryClasses', 'AlgorithmQuality', NULL, NULL, NULL, NULL, NULL),
('HandlesNumericFeatures', 'AlgorithmQuality', NULL, NULL, NULL, NULL, NULL),
('HandlesNumericTarget', 'AlgorithmQuality', NULL, NULL, NULL, NULL, NULL),
('JointEntropy', 'DataQuality', NULL, 'Joint entropies of every attribute and the class attribute. Usually, the min,max and mean are calculated. The joint entropy defines how much information is shared between each attribute and the class attribute. It will be zero if the attributes are independent. As such, this measure determines the level of dependence between the attributes and the class attribute. ', NULL, NULL, NULL),
('LandMarker1NN', 'DataQuality', 'See PredictiveAccuracy and weka.IB1', 'Predictive accuracy of WEKA''s Nearest Neighbor algorithm with a single neighbor (k=1). Determines how close instances of the same class are.\r\n\r\nSee Pfahringer et al. (2000) ''Meta-learning by landmarking various learning algorithms''. ICML 2000.', NULL, NULL, NULL),
('LandMarker1Rule', 'DataQuality', 'See PredictiveAccuracy and weka.OneR', 'Predictive accuracy of WEKA''s 1-Rule algorithm. Determines how much information is contained in the most predictive attribute.', NULL, NULL, NULL),
('LandmarkerNaiveBayes', 'DataQuality', 'See PredictiveAccuracy and weka.NaiveBayes', 'Predictive accuracy of WEKA''s Naive Bayes algorithm with default parameter settings. Determines to what extent the features are conditionally independent. \r\n\r\nSee Pfahringer et al. (2000) ''Meta-learning by landmarking various learning algorithms''. ICML 2000.', NULL, NULL, NULL),
('MStatistic', 'DataQuality', 'In the following, $S_{i}=\\frac{S_{c_{i}}}{n_{i}-1}$ is the $i$ class covariance matrix with $S_{c_{i}}$ the $i$ class scatter matrix and $n_{i}$ the number of examples pertaining to class $i$, and $S=\\frac{1}{n-cl}\\sum_{i}S_{c_{i}}$ the pooled covariance matrix. It is zero when all individual covariance matrices are equal to the pooled covariance matrix.\r\n\\begin{eqnarray}\r\nM&=&\\gamma\\sum_{i}(n_{i}-1)log\\frac{|S|}{|S_{i}|} \\\\\r\n\\gamma&=&1-\\frac{2num^{2}+3num-1}{6(num+1)(cl-1)}(\\sum_{i}\\frac{1}{n_{i}-1}-\\frac{1}{n-cl})\r\n\\end{eqnarray}', 'Box''s M-statistic measures the equality of the covariance matrices of the different classes. If they are equal, then linear discriminants could be used, otherwise, quadratic discriminant functions should be used instead. As such, the M-statictic predicts whether a linear discriminant algorithm should be used or not.', NULL, NULL, NULL),
('MStatisticChiSquared', 'DataQuality', NULL, 'Chi-squared distribution of the M-statistic over all features.', NULL, NULL, NULL),
('MStatisticDegreesOfFreedom', 'DataQuality', NULL, 'Degrees of freedom of the chi-squared distribution of the M-statistic over all features.', NULL, NULL, NULL),
('MutualInformationMean', 'DataQuality', '\\overline{MI(C,X)}=\\frac{\\sum_{i=1}^{attr}MI(C,X_{i})}{attr}\r\nMI(Y,X)&=&H(Y)-H(Y|X) \\\\\r\nH(Y|X)&=&\\sum_{i}p(X=x_{i})H(Y|X=x_{i}) \\\\\r\n&=&-\\sum_{i}\\pi_{i+} \\sum_{j}\\pi_{j|i}log_{2}(\\pi_{j|i})\r\n', 'Mean of the mutual information between every attribute and the class attribute.\r\n\r\nIt describes the reduction in uncertainty of the class due to the knowledge of th attribute. Mutual information is also the underlying measure of the information gain metric used in decision tree learners. ', NULL, NULL, NULL),
('NoiseToSignalRatio', 'DataQuality', 'NS\\mbox{-}ratio=\\frac{\\overline{H(X)}-\\overline{MI(C,X)}}{\\overline{MI(C,X)}}\r\n\r\n$\\overline{H(X)}$ is the average information (useful or not) of the attributes.', 'An estimate of the amount of non-useful information in the attributes regarding the class.\r\n', NULL, NULL, NULL),
('NumberOfClasses', 'DataQuality', 'count(classes)', 'The number of classes in the class attribute.', NULL, NULL, NULL),
('NumberOfFeatures', 'DataQuality', 'count(features)', 'The number of features (attributes) in the dataset. Also known as the dimensionality of the dataset.', NULL, NULL, NULL),
('NumberOfInstances', 'DataQuality', 'Count(instances)', 'The number of instances (examples) in the database.', NULL, NULL, NULL),
('NumberOfInstancesWithMissingValues', 'DataQuality', 'count(instances with missing values)', 'Counts the number of instances that contain missing values.', NULL, NULL, NULL),
('NumberOfMissingValues', 'DataQuality', 'count(missing values)', 'Counts the total number of missing values in the dataset.', NULL, NULL, NULL),
('NumberOfNumericFeatures', 'DataQuality', 'count(numeric features)', 'The number of symbolic features in the dataset.', NULL, NULL, NULL),
('NumberOfSymbolicFeatures', 'DataQuality', 'count(symbolic features)', 'The number of symbolic features in the dataset.', NULL, NULL, NULL),
('PerformsClassification', 'AlgorithmQuality', NULL, 'true if the algorithm can perform classification, false otherwise', NULL, NULL, NULL),
('PerformsRegression', 'AlgorithmQuality', NULL, 'true if the algorithm can perform regression, false otherwise', NULL, NULL, NULL),
('PublicationDate', 'DataQuality', 'The date this dataset was submitted to the UCI repository.', NULL, NULL, NULL, NULL),
('PublicationYear', 'DataQuality', NULL, 'The year the dataset was published.', NULL, NULL, NULL),
('StandardDeviationRatio', 'DataQuality', 'SD\\mbox{-}ratio=exp(\\frac{M}{num\\sum_{i}(n_{i}-1)})\r\nM: see MStatistic', 'The standard deviation ratio is a reexpression of MStatistic, which is one if MStatistic is zero and strictly greater than one if the covariances differ.', NULL, NULL, NULL),
('UCIRepositoryURL', 'DataQuality', NULL, 'URL of this dataset in the UCI repository', NULL, NULL, NULL),
('VarianceWeightKohaviWolpert', 'AlgorithmQuality', '', 'empirically calculated average ratio of variance error in the total error, using Kohavi-Wolpert''s definition of bias and variance', NULL, NULL, NULL),
('VarianceWeightWebb', 'AlgorithmQuality', NULL, 'empirically determined average ratio of variance error in the total error, using Webb''s definition of bias and variance', NULL, NULL, NULL);
